{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Dict\n",
    "import logging\n",
    "import torch\n",
    "from torch import optim\n",
    "import pickle\n",
    "from datasets import TemporalDataset\n",
    "from optimizers import TKBCOptimizer, IKBCOptimizer\n",
    "from models import ComplEx, TComplEx, TNTComplEx\n",
    "from regularizers import N3, Lambda3\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume all timestamps are regularly spaced\n",
      "Not using time intervals and events eval\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# create model\n",
    "# this resets the model\n",
    "\n",
    "import argparse\n",
    "from typing import Dict\n",
    "import logging\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from datasets import TemporalDataset\n",
    "from optimizers import TKBCOptimizer, IKBCOptimizer\n",
    "from models import ComplEx, TComplEx, TNTComplEx\n",
    "from regularizers import N3, Lambda3\n",
    "\n",
    "DATASET_NAME = 'MultiTQ'\n",
    "class Args:\n",
    "    dataset =  '../../data/'+DATASET_NAME+'/kg/tkbc_processed_data'\n",
    "    #dataset =  '../../data/'+DATASET_NAME+'/kg/tkbc_processed_data'\n",
    "    model =  'TComplEx'\n",
    "    max_epochs = 200\n",
    "    valid_freq = 20\n",
    "    rank = 256\n",
    "    batch_size = 1024\n",
    "    learning_rate = 0.1\n",
    "    emb_reg = 0.001\n",
    "    time_reg = 0.001\n",
    "    no_time_emb = False\n",
    "    \n",
    "args=Args()\n",
    "\n",
    "dataset = TemporalDataset(args.dataset)\n",
    "\n",
    "sizes = dataset.get_shape()\n",
    "model = {\n",
    "    'ComplEx': ComplEx(sizes, args.rank),\n",
    "    'TComplEx': TComplEx(sizes, args.rank, no_time_emb=args.no_time_emb),\n",
    "    'TNTComplEx': TNTComplEx(sizes, args.rank, no_time_emb=args.no_time_emb),\n",
    "}[args.model]\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "opt = optim.Adagrad(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "emb_reg = N3(args.emb_reg)\n",
    "time_reg = Lambda3(args.time_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume all timestamps are regularly spaced\n",
      "Not using time intervals and events eval\n"
     ]
    }
   ],
   "source": [
    "dataset = TemporalDataset(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|██████████| 737924/737924 [00:08<00:00, 88123.33ex/s, cont=0, loss=4, reg=0]\n",
      "train loss: 100%|██████████| 737924/737924 [00:09<00:00, 79081.52ex/s, cont=0, loss=4, reg=0]\n",
      "train loss: 100%|██████████| 737924/737924 [00:08<00:00, 82656.49ex/s, cont=0, loss=4, reg=0]\n",
      "train loss: 100%|██████████| 737924/737924 [00:11<00:00, 62366.24ex/s, cont=0, loss=4, reg=0] \n",
      "train loss: 100%|██████████| 737924/737924 [00:19<00:00, 37978.25ex/s, cont=0, loss=3, reg=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5059451311826706\n",
      "test:  0.5054048001766205\n",
      "train:  0.9835851788520813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 100%|██████████| 737924/737924 [00:05<00:00, 133943.13ex/s, cont=0, loss=4, reg=0]\n",
      "train loss: 100%|██████████| 737924/737924 [00:05<00:00, 135228.60ex/s, cont=0, loss=4, reg=0]\n",
      "train loss: 100%|██████████| 737924/737924 [00:05<00:00, 131247.21ex/s, cont=0, loss=3, reg=0]\n",
      "train loss: 100%|██████████| 737924/737924 [00:05<00:00, 143963.70ex/s, cont=0, loss=3, reg=0]\n",
      "train loss: 100%|██████████| 737924/737924 [00:05<00:00, 140531.79ex/s, cont=0, loss=3, reg=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  0.5010942667722702\n",
      "test:  0.5007961839437485\n",
      "train:  0.9892488420009613\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(10):\n",
    "    examples = torch.from_numpy(\n",
    "        dataset.get_train().astype('int64')\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    if dataset.has_intervals():\n",
    "        optimizer = IKBCOptimizer(\n",
    "            model, emb_reg, time_reg, opt, dataset,\n",
    "            batch_size=args.batch_size\n",
    "        )\n",
    "        optimizer.epoch(examples)\n",
    "\n",
    "    else:\n",
    "        optimizer = TKBCOptimizer(\n",
    "            model, emb_reg, time_reg, opt,\n",
    "            batch_size=args.batch_size\n",
    "        )\n",
    "        optimizer.epoch(examples)\n",
    "\n",
    "\n",
    "    def avg_both(mrrs: Dict[str, float], hits: Dict[str, torch.FloatTensor]):\n",
    "        \"\"\"\n",
    "        aggregate metrics for missing lhs and rhs\n",
    "        :param mrrs: d\n",
    "        :param hits:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        m = (mrrs['lhs'] + mrrs['rhs']) / 2.\n",
    "        h = (hits['lhs'] + hits['rhs']) / 2.\n",
    "        return {'MRR': m, 'hits@[1,3,10]': h}\n",
    "\n",
    "    if epoch < 0 or (epoch + 1) % 5 == 0:\n",
    "        if dataset.has_intervals():\n",
    "            valid, test, train = [\n",
    "                dataset.eval(model, split, -1 if split != 'train' else 50000)\n",
    "                for split in ['valid', 'test', 'train']\n",
    "            ]\n",
    "            print(\"valid: \", valid)\n",
    "            print(\"test: \", test)\n",
    "            print(\"train: \", train)\n",
    "\n",
    "        else:\n",
    "            valid, test, train = [\n",
    "                avg_both(*dataset.eval(model, split, -1 if split != 'train' else 50000))\n",
    "                for split in ['valid', 'test', 'train']\n",
    "            ]\n",
    "            print(\"valid: \", valid['MRR'])\n",
    "            print(\"test: \", test['MRR'])\n",
    "            print(\"train: \", train['MRR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path = '../../Baseline仓库/code/models/kg_embeddings/tcomplex_new.ckpt'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path = '../models/'+DATASET_NAME+'/kg_embeddings/enhanced_kg_with_time.ckpt'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'tkbc_model/'+DATASET_NAME+'/kg_embeddings/icews_enhanced_v.ckpt'\n",
    "x = torch.load(path)\n",
    "model.load_state_dict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTime(question, model, all_dicts, k=1):\n",
    "    entities = list(question['entities'])\n",
    "    times = question['time']\n",
    "    target_type = 'simple_entity'\n",
    "    ent2id = all_dicts['ent2id']\n",
    "    rel2id = all_dicts['rel2id']\n",
    "    ts2id = all_dicts['ts2id']\n",
    "    id2ent = all_dicts['id2ent']\n",
    "    id2ts = all_dicts['id2ts']\n",
    "    head = ent2id[entities[0]]\n",
    "    tail = ent2id[entities[1]]\n",
    "    relation = question['relations']\n",
    "    relation = rel2id[relation]  # + model.embeddings[1].weight.shape[0]//2 #+ 90\n",
    "    data_point = [head, relation, tail, 1]\n",
    "    data_batch = torch.from_numpy(np.array([data_point])).cuda()\n",
    "    time_scores = model.forward_over_time(data_batch)\n",
    "    val, ind = torch.topk(time_scores, k, dim=1)\n",
    "    topk_set = set()\n",
    "    for row in ind:\n",
    "        for x in row:\n",
    "            topk_set.add(id2ts[x.item()])\n",
    "    return topk_set\n",
    "\n",
    "\n",
    "def predictTail(question, model, all_dicts, k=1):\n",
    "    entities = list(question['entities'])\n",
    "    times = question['time']\n",
    "    target_type = 'simple_entity'\n",
    "    ent2id = all_dicts['ent2id']\n",
    "    rel2id = all_dicts['rel2id']\n",
    "    ts2id = all_dicts['ts2id']\n",
    "    id2ent = all_dicts['id2ent']\n",
    "    id2ts = all_dicts['id2ts']\n",
    "    head = ent2id[entities[0]]\n",
    "    time = ts2id[times[0]]\n",
    "    relation = question['relations']\n",
    "    relation = rel2id[relation]  # + model.embeddings[1].weight.shape[0]//2 #+ 90\n",
    "    data_point = [head, relation, 1, time]\n",
    "    data_batch = torch.from_numpy(np.array([data_point])).cuda()\n",
    "    predictions, factors, time = model.forward(data_batch)\n",
    "    val, ind = torch.topk(predictions, k, dim=1)\n",
    "    topk_set = set()\n",
    "    for row in ind:\n",
    "        for x in row:\n",
    "            topk_set.add(id2ent[x.item()])\n",
    "    return topk_set\n",
    "\n",
    "\n",
    "def checkIfTkbcEmbeddingsTrained(tkbc_model, split='test'):\n",
    "    with open('../data/MultiTQ/questions/full_data/'+split+'.json') as f:\n",
    "        questions = json.load(f)\n",
    "    question_type ='equal'\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    k = 1  # hit at k\n",
    "    for i in tqdm(range(len(questions))):\n",
    "        this_question_type = questions[i]['qtype']\n",
    "        if question_type == this_question_type and questions[i]['answer_type'] == 'entity' and questions[i]['time_level'] == 'day':\n",
    "            which_question_function = predictTail\n",
    "        elif question_type == this_question_type and questions[i]['answer_type'] == 'time' and questions[i]['time_level'] == 'day':\n",
    "            which_question_function = predictTime\n",
    "        else:\n",
    "            continue\n",
    "        total_count += 1\n",
    "        id = i\n",
    "        predicted = which_question_function(questions[id], tkbc_model, all_dicts, k)\n",
    "        intersection_set = set(questions[id]['answers']).intersection(predicted)\n",
    "        if len(intersection_set) > 0:\n",
    "            correct_count += 1\n",
    "    print(question_type, correct_count, total_count, correct_count / total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getAllDicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ddf7d425e7eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAllDicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcheckIfTkbcEmbeddingsTrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtkbc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getAllDicts' is not defined"
     ]
    }
   ],
   "source": [
    "all_dicts = getAllDicts(args.dataset_name,args.kg_dir)\n",
    "checkIfTkbcEmbeddingsTrained(tkbc_model, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
